{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Master Degree in Computer Science and Data Science for Economics\n",
    "\n",
    "# Word2Vec resources\n",
    "## Example of using W2V to check for semantic shifts\n",
    "\n",
    "### Alfio Ferrara\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main functionalities of `gensim` implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pymongo.MongoClient()['cousine']\n",
    "recipes = db['foodcom']\n",
    "\n",
    "q = {}\n",
    "recipe_corpus = []\n",
    "size = recipes.count_documents(q)\n",
    "limit = 50_000\n",
    "\n",
    "for recipe in recipes.find(q).limit(limit):\n",
    "    try:\n",
    "        recipe_corpus.append(word_tokenize(recipe['description'].lower()))\n",
    "    except TypeError:\n",
    "        pass \n",
    "    except AttributeError:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'love', 'grits', ',', 'this', 'is', 'another', 'good', 'way', 'to', 'serve', 'them', '.', 'a', 'great', 'alternative', 'to', 'a', 'baked', 'potato', 'when', 'served', 'with', 'grilled', 'steak', 'or', 'chicken', '.', 'i', 'belive', 'this', 'recipe', 'could', 'be', 'made', 'with', 'instant', 'grits.the', '2', '1/2', 'hours', 'for', 'refrigeration', 'is', 'not', 'include', 'in', 'time', '.', 'the', 'recipe', 'comes', 'from', 'tast', 'of', 'home', \"'s\", 'light', 'and', 'tasty', '.']\n"
     ]
    }
   ],
   "source": [
    "print(recipe_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_model = Word2Vec(sentences=recipe_corpus, vector_size=300, window=5, \n",
    "                        min_count=1, workers=8, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('noodles', 0.5552150011062622),\n",
       " ('fettuccine', 0.5019420385360718),\n",
       " ('spaghetti', 0.48538538813591003),\n",
       " ('lasagna', 0.470816045999527),\n",
       " ('greens', 0.4468240737915039),\n",
       " ('broccoli', 0.4334532618522644),\n",
       " ('couscous', 0.4136292338371277),\n",
       " ('polenta', 0.4132252335548401),\n",
       " ('steamed', 0.40818169713020325),\n",
       " ('linguine', 0.4080011546611786)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_model.wv.most_similar('pasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compositionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = recipe_model.wv.doesnt_match(['pasta', 'spaghetti', 'noodles', 'apple'])\n",
    "common = recipe_model.wv.get_mean_vector(['pasta', 'spaghetti', 'noodles', 'risotto'])\n",
    "common_word = recipe_model.wv.similar_by_vector(common)\n",
    "analogy = recipe_model.wv.most_similar(positive=['pizza', 'steak'], negative=['tomato'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doesn't match: apple\n",
      "Common terms: [('noodles', 0.7980520725250244), ('pasta', 0.7943751811981201), ('spaghetti', 0.7324199080467224), ('lasagna', 0.6158578395843506), ('risotto', 0.6023666858673096), ('polenta', 0.5509158372879028), ('fettuccine', 0.5406956672668457), ('linguine', 0.5256999731063843), ('penne', 0.5146601796150208), ('couscous', 0.4943198263645172)]\n",
      "Analogy: [('steaks', 0.39486193656921387), ('grill', 0.368589848279953), ('bbq', 0.36669859290122986), ('burger', 0.35022082924842834), ('lasagna', 0.33507680892944336), ('ribs', 0.3340110778808594), ('grilled', 0.32294386625289917), ('roast', 0.3165149688720703), ('fork', 0.3116554319858551), ('sirloin', 0.31139659881591797)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Doesn't match: {dm}\")\n",
    "print(f\"Common terms: {common_word}\")\n",
    "print(f\"Analogy: {analogy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare models vectors to measure a shift in meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_tv = pymongo.MongoClient()['tmdb']\n",
    "tvseries = db_tv['tvseries']\n",
    "\n",
    "q = {}\n",
    "tv_corpus = []\n",
    "size = tvseries.count_documents(q)\n",
    "limit = 50_000\n",
    "\n",
    "for tvs in tvseries.find(q).limit(limit):\n",
    "    try:\n",
    "        tv_corpus.append(word_tokenize(tvs['overview'].lower()))\n",
    "    except TypeError:\n",
    "        pass \n",
    "    except AttributeError:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walter', 'white', ',', 'a', 'new', 'mexico']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_corpus[0][:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_model = Word2Vec(sentences=tv_corpus, vector_size=100, window=5, \n",
    "                        min_count=1, workers=8, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('redeem', 0.9673093557357788),\n",
       " ('loveless', 0.9648919701576233),\n",
       " ('mission', 0.9624361395835876),\n",
       " ('elder', 0.961053729057312),\n",
       " ('parents', 0.9608786702156067),\n",
       " ('problematic', 0.959640622138977),\n",
       " ('keung', 0.9570775628089905),\n",
       " ('superheroes', 0.9566478729248047),\n",
       " ('enhancing', 0.9562162160873413),\n",
       " ('peeping', 0.9530292749404907)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_model.wv.most_similar('brother')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('boyfriend', 0.7582463026046753),\n",
       " ('sister', 0.7582341432571411),\n",
       " ('daughter', 0.7382907271385193),\n",
       " ('fiance', 0.7378897070884705),\n",
       " ('dad', 0.7274300456047058),\n",
       " ('niece', 0.7191648483276367),\n",
       " ('wife', 0.7175347208976746),\n",
       " ('dd', 0.7085770964622498),\n",
       " ('father', 0.7035520076751709),\n",
       " ('son', 0.6993942260742188)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_model.wv.most_similar('brother')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring semantic shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italian corpus: 4920, Chinese corpus: 4871\n"
     ]
    }
   ],
   "source": [
    "italian_q = {'search_terms': 'italian'}\n",
    "chinese_q = {'search_terms': 'chinese'}\n",
    "limit = 5_000\n",
    "italian_corpus = []\n",
    "chinese_corpus = []\n",
    "\n",
    "for q, c in [(italian_q, italian_corpus), (chinese_q, chinese_corpus)]:\n",
    "    for doc in recipes.find(q).limit(limit):\n",
    "        try:\n",
    "            tokens = word_tokenize(doc['description'].lower())\n",
    "            c.append(tokens)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "print(f\"Italian corpus: {len(italian_corpus)}, Chinese corpus: {len(chinese_corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_corpus = italian_corpus + chinese_corpus\n",
    "m0 = Word2Vec(sentences=main_corpus, vector_size=100, window=5, \n",
    "                        min_count=1, workers=8, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spaghetti', 0.5600408911705017),\n",
       " ('penne', 0.45347607135772705),\n",
       " ('linguine', 0.4507941007614136),\n",
       " ('fish', 0.4485734701156616),\n",
       " ('hair', 0.440589964389801),\n",
       " ('fettuccine', 0.4274340867996216),\n",
       " ('meat', 0.42419877648353577),\n",
       " ('tomato', 0.41989225149154663),\n",
       " ('risotto', 0.4197191894054413),\n",
       " ('ziti', 0.41755545139312744)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m0.wv.most_similar('pasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune the global model to specific sub-corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_it = copy.deepcopy(m0)\n",
    "m_ch = copy.deepcopy(m0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7021719, 9992950)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_it.train(italian_corpus, total_examples=m0.corpus_count, epochs=m0.epochs)\n",
    "m_ch.train(chinese_corpus, total_examples=m0.corpus_count, epochs=m0.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('soup', 0.5136364698410034),\n",
       " ('sauces', 0.4332062304019928),\n",
       " ('dish', 0.430277943611145),\n",
       " ('chicken-you', 0.416638046503067),\n",
       " ('chunky', 0.4089588522911072),\n",
       " ('marinade/sauce', 0.40361347794532776),\n",
       " ('broth', 0.40259942412376404),\n",
       " ('meatballs', 0.3962996304035187),\n",
       " ('paste', 0.3920050859451294),\n",
       " ('time-tested', 0.38308626413345337)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_it.wv.most_similar('sauce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paste', 0.4618358314037323),\n",
       " ('wedges', 0.42947617173194885),\n",
       " ('paste/sauce', 0.4207182824611664),\n",
       " ('soup', 0.40637680888175964),\n",
       " ('chunky', 0.3986596167087555),\n",
       " ('marinade', 0.38860464096069336),\n",
       " ('dark', 0.38331976532936096),\n",
       " (\"'dark\", 0.378656804561615),\n",
       " ('time-tested', 0.3783089816570282),\n",
       " ('rich/thick', 0.3740333616733551)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_ch.wv.most_similar('sauce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'spaghetti'\n",
    "v0, vit, vch = m0.wv.get_vector(word), m_it.wv.get_vector(word), m_ch.wv.get_vector(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving to IT: 0.09616337660618668\n",
      "Moving to CH: 0.055406030742616186\n"
     ]
    }
   ],
   "source": [
    "print(f\"Moving to IT: {distance.cosine(vit, v0)}\")\n",
    "print(f\"Moving to CH: {distance.cosine(vch, v0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gensim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
