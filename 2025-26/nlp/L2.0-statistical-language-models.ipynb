{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Master Degree in Computer Science and Data Science for Economics\n",
    "\n",
    "# Statistical Language Models\n",
    "\n",
    "### Alfio Ferrara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a corpus $D$ of size $N$, we can compute the probability of a token $w \\in D$ as:\n",
    "\n",
    "$$\n",
    "P(w) = \\frac{count(w)}{\\sum\\limits_{i=0}^{N} count(w_i)}\n",
    "$$\n",
    "\n",
    "Using this simple statistics, we can sample a word from the corpus according to its probability $P(w)$. If we use this to generate a text $d = w_1, w_2, \\dots, w_{n-1}, w_n$ we have:\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, \\dots, w_{n-1}, w_n) = P(w_1)P(w_2) \\dots P(w_{n-1})P(w_n) = \\prod\\limits_{i=1}^{n} P(w_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Howewer, this is not realistic. A better way to model this process is to choose words by taking into account the words we generated before, by sampling the $i$th word with a probabiity that is conditioned by the previous $i-1$ words.\n",
    "\n",
    "Thus, applying the chain rule:\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, \\dots, w_{n-1}, w_n) = P(w_1) P(w_2 \\mid w_1) \\dots P(w_{n-1} \\mid w_1 \\dots w_{n-2}) P(w_n \\mid w_1 \\dots w_{n-1}) = \\prod\\limits_{i=1}^{n} P(w_i \\mid w_1 \\dots w_{i-1})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, indexing long sequences as required by this method is unfeasible for a couple of good reasons:\n",
    "1. it's memory consuming and computationally intractable\n",
    "2. The logest the sequences the fewer the chances to observe such sequences a sufficient number of times\n",
    "\n",
    "So, we can apply a Markov approximation by taking into account only subsequences of lenght $k$:\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, \\dots, w_{n-1}, w_n) = \\prod\\limits_{i=1}^{n} P(w_i \\mid w_{i-k} \\dots w_{i-1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp.langmodel import MarkovLM\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a corpus of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pymongo.MongoClient()['cousine']\n",
    "recipes = db['foodcom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(query:  dict = {}, numdocs: int = 3000):\n",
    "    corpus = []\n",
    "    for recipe in recipes.find(query).limit(numdocs):\n",
    "        for sentence in recipe['steps']:\n",
    "            corpus.append(sentence)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 20262\n",
      "I a sauce pan, bring water to a boil; slowly add grits and salt, stirring constantly; Reduce heat:simmer, uncovered, for 40-45 minutes or untill thickened, stirrin occasionally.\n",
      "Add cheese and garlic; stir until cheese is melted, Spray 9-inch baking dish with nonstick cooking spray; Cover and refrigerate for 2 to 2 1/2 hours or until frim.\n",
      "Before starting the grill, coat the grill rack with nonstick cooking spray; Cut the grits into 3-inch squares; Brush both sides with olive oil.\n",
      "Grill, covered, over medium heat for 4 to 6 minutes on each side or until lightly browned.\n"
     ]
    }
   ],
   "source": [
    "numdocs = 3000\n",
    "corpus = create_corpus(query={}, numdocs=numdocs)\n",
    "print(f\"Corpus size: {len(corpus)}\")\n",
    "for text in corpus[:4]:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = \"bert-base-uncased\"\n",
    "brlm = MarkovLM(k=2, tokenizer_model=tokenizer)\n",
    "frlm = MarkovLM(k=4, tokenizer_model=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20262/20262 [00:02<00:00, 8247.65it/s]\n",
      "100%|██████████| 20262/20262 [00:02<00:00, 7249.22it/s]\n"
     ]
    }
   ],
   "source": [
    "brlm.train(corpus=corpus)\n",
    "frlm.train(corpus=corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[#S]', 'slice', 'one', 'of', 'sour', 'cream', 'cheese', '.', '[#E]']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brlm.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2gram:  [#S] your pick comes together to 40 minutes . [#E]\n",
      "4gram:  [#S] [#S] [#S] heat butter in a 12 - cup bundt pan and bake until syrup thickens and strawberries on each layer . [#E]\n"
     ]
    }
   ],
   "source": [
    "print(\"2gram: \", \" \".join(brlm.generate()).replace(\" ##\", \"\"))\n",
    "print(\"4gram: \", \" \".join(frlm.generate()).replace(\" ##\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italian: 30052\n",
      "Chinese: 24134\n"
     ]
    }
   ],
   "source": [
    "italian_q = {'search_terms': 'italian'}\n",
    "chinese_q = {'search_terms': 'chinese'}\n",
    "numdocs = 3000\n",
    "italian_corpus = create_corpus(query=italian_q, numdocs=numdocs)\n",
    "chinese_corpus = create_corpus(query=chinese_q, numdocs=numdocs)\n",
    "print(f\"Italian: {len(italian_corpus)}\")\n",
    "print(f\"Chinese: {len(chinese_corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = MarkovLM(k=4, tokenizer_model=tokenizer)\n",
    "ch = MarkovLM(k=4, tokenizer_model=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30052/30052 [00:04<00:00, 7468.25it/s]\n",
      "100%|██████████| 24134/24134 [00:03<00:00, 6670.37it/s]\n"
     ]
    }
   ],
   "source": [
    "it.train(corpus=italian_corpus)\n",
    "ch.train(corpus=chinese_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italian sentence: Place ravioli on a large baking sheet sprinkled with cornstarch.\n",
      "Italian: -23.0900057470743\n",
      "Chinese: -21.352223087931076\n",
      "========\n",
      "Chinese sentence: Heat enough oil in a frying pan over medium heat to shallow fry.\n",
      "Italian: -24.643378642963395\n",
      "Chinese: -22.686133859693925\n"
     ]
    }
   ],
   "source": [
    "italian_sentence = italian_corpus[6]\n",
    "chinese_sentence = chinese_corpus[6]\n",
    "\n",
    "print(f\"Italian sentence: {italian_sentence}\")\n",
    "print(f\"Italian: {it.log_prob(italian_sentence)}\")\n",
    "print(f\"Chinese: {ch.log_prob(italian_sentence)}\")\n",
    "print(\"========\")\n",
    "print(f\"Chinese sentence: {chinese_sentence}\")\n",
    "print(f\"Italian: {it.log_prob(chinese_sentence)}\")\n",
    "print(f\"Chinese: {ch.log_prob(chinese_sentence)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine languages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix = copy.deepcopy(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24134/24134 [00:03<00:00, 6457.06it/s]\n"
     ]
    }
   ],
   "source": [
    "mix.train(chinese_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mix:  [#S] [#S] [#S] squash some of the cream mixture to frying pan over med - high ) , then add the onions and bouquet garni , simply tie up the herbs you select in a piece of chinese sausages are lightly browned . [#E]\n"
     ]
    }
   ],
   "source": [
    "print(\"Mix: \", \" \".join(mix.generate()).replace(\" ##\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
