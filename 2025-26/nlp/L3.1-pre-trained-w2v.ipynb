{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Master Degree in Computer Science and Data Science for Economics\n",
    "\n",
    "# Word2Vec resources\n",
    "## Example of using W2V to check for semantic shifts\n",
    "\n",
    "### Alfio Ferrara\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main functionalities of `gensim` implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pymongo.MongoClient()['cousine']\n",
    "recipes = db['foodcom']\n",
    "\n",
    "q = {}\n",
    "recipe_corpus = []\n",
    "size = recipes.count_documents(q)\n",
    "limit = 50_000\n",
    "\n",
    "for recipe in recipes.find(q).limit(limit):\n",
    "    try:\n",
    "        recipe_corpus.append(word_tokenize(recipe['description'].lower()))\n",
    "    except TypeError:\n",
    "        pass \n",
    "    except AttributeError:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'love', 'grits', ',', 'this', 'is', 'another', 'good', 'way', 'to', 'serve', 'them', '.', 'a', 'great', 'alternative', 'to', 'a', 'baked', 'potato', 'when', 'served', 'with', 'grilled', 'steak', 'or', 'chicken', '.', 'i', 'belive', 'this', 'recipe', 'could', 'be', 'made', 'with', 'instant', 'grits.the', '2', '1/2', 'hours', 'for', 'refrigeration', 'is', 'not', 'include', 'in', 'time', '.', 'the', 'recipe', 'comes', 'from', 'tast', 'of', 'home', \"'s\", 'light', 'and', 'tasty', '.']\n"
     ]
    }
   ],
   "source": [
    "print(recipe_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_model = Word2Vec(sentences=recipe_corpus, vector_size=300, window=5, \n",
    "                        min_count=1, workers=8, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('supper', 0.6715042591094971),\n",
       " ('meal', 0.5441684126853943),\n",
       " ('brunch', 0.4939667582511902),\n",
       " ('appetizer', 0.4601024091243744),\n",
       " ('lunch', 0.4385051131248474),\n",
       " ('entertaining', 0.43703317642211914),\n",
       " ('dinners', 0.43435272574424744),\n",
       " ('entree', 0.43312951922416687),\n",
       " ('picnic', 0.42357006669044495),\n",
       " ('gathering', 0.42194387316703796)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_model.wv.most_similar('dinner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compositionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = recipe_model.wv.doesnt_match(['pasta', 'spaghetti', 'noodles', 'apple'])\n",
    "common = recipe_model.wv.get_mean_vector(['pasta', 'spaghetti', 'noodles', 'risotto'])\n",
    "common_word = recipe_model.wv.similar_by_vector(common)\n",
    "analogy = recipe_model.wv.most_similar(positive=['pizza', 'steak'], negative=['tomato'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doesn't match: apple\n",
      "Common terms: [('noodles', 0.7999148368835449), ('pasta', 0.7884016633033752), ('spaghetti', 0.7322816252708435), ('risotto', 0.5933534502983093), ('lasagna', 0.5874766111373901), ('polenta', 0.5733187198638916), ('linguine', 0.5356707572937012), ('penne', 0.5232851505279541), ('fettuccine', 0.5186668634414673), ('couscous', 0.516014039516449)]\n",
      "Analogy: [('steaks', 0.3971555233001709), ('burger', 0.35433799028396606), ('grill', 0.3518954813480377), ('lasagna', 0.34866949915885925), ('ribs', 0.33822542428970337), ('roast', 0.3349013328552246), ('bbq', 0.3304198980331421), ('flank', 0.3288681209087372), ('15-inch', 0.32589617371559143), ('grilled', 0.32188883423805237)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Doesn't match: {dm}\")\n",
    "print(f\"Common terms: {common_word}\")\n",
    "print(f\"Analogy: {analogy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare models vectors to measure a shift in meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_tv = pymongo.MongoClient()['tmdb']\n",
    "tvseries = db_tv['tvseries']\n",
    "\n",
    "q = {}\n",
    "tv_corpus = []\n",
    "size = tvseries.count_documents(q)\n",
    "limit = 50_000\n",
    "\n",
    "for tvs in tvseries.find(q).limit(limit):\n",
    "    try:\n",
    "        tv_corpus.append(word_tokenize(tvs['overview'].lower()))\n",
    "    except TypeError:\n",
    "        pass \n",
    "    except AttributeError:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walter', 'white', ',', 'a', 'new', 'mexico']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_corpus[0][:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_model = Word2Vec(sentences=tv_corpus, vector_size=300, window=5, \n",
    "                        min_count=1, workers=8, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sect', 0.9945293068885803),\n",
       " ('ultimate', 0.9929512739181519),\n",
       " ('west', 0.9923814535140991),\n",
       " ('shine', 0.9923009872436523),\n",
       " ('greedy', 0.9919392466545105),\n",
       " ('head', 0.9914314150810242),\n",
       " ('never-before-told', 0.991184651851654),\n",
       " ('holds', 0.9907960891723633),\n",
       " ('leaving', 0.9907811880111694),\n",
       " ('secrets', 0.9905073642730713)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_model.wv.most_similar('dinner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sister', 0.7628988027572632),\n",
       " ('daughter', 0.7540379166603088),\n",
       " ('boyfriend', 0.7494955658912659),\n",
       " ('dad', 0.7205260992050171),\n",
       " ('father', 0.7112997174263),\n",
       " ('brother-in-law', 0.7054951786994934),\n",
       " ('wife', 0.7037829160690308),\n",
       " ('fiance', 0.7002346515655518),\n",
       " ('dd', 0.6990201473236084),\n",
       " ('niece', 0.6981290578842163)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_model.wv.most_similar('brother')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring semantic shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italian corpus: 4920, Chinese corpus: 4871\n"
     ]
    }
   ],
   "source": [
    "italian_q = {'search_terms': 'italian'}\n",
    "chinese_q = {'search_terms': 'chinese'}\n",
    "limit = 5_000\n",
    "italian_corpus = []\n",
    "chinese_corpus = []\n",
    "\n",
    "for q, c in [(italian_q, italian_corpus), (chinese_q, chinese_corpus)]:\n",
    "    for doc in recipes.find(q).limit(limit):\n",
    "        try:\n",
    "            tokens = word_tokenize(doc['description'].lower())\n",
    "            c.append(tokens)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "print(f\"Italian corpus: {len(italian_corpus)}, Chinese corpus: {len(chinese_corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_corpus = italian_corpus + chinese_corpus\n",
    "m0 = Word2Vec(sentences=main_corpus, vector_size=100, window=5, \n",
    "                        min_count=1, workers=8, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('meal', 0.6804056167602539),\n",
       " ('supper', 0.6573017835617065),\n",
       " ('lunch', 0.5930486917495728),\n",
       " ('dinners', 0.5533140897750854),\n",
       " ('night', 0.5517100691795349),\n",
       " ('snack', 0.5194377303123474),\n",
       " ('starter', 0.4874492585659027),\n",
       " ('cocktail', 0.48230254650115967),\n",
       " ('picnics', 0.47330382466316223),\n",
       " ('guests', 0.4727531969547272)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m0.wv.most_similar('dinner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune the global model to specific sub-corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_it = copy.deepcopy(m0)\n",
    "m_ch = copy.deepcopy(m0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7022315, 9992950)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_it.train(italian_corpus, total_examples=m0.corpus_count, epochs=m0.epochs)\n",
    "m_ch.train(chinese_corpus, total_examples=m0.corpus_count, epochs=m0.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('meal', 0.6211058497428894),\n",
       " ('supper', 0.5325251221656799),\n",
       " ('snack', 0.5054898262023926),\n",
       " ('cocktail', 0.4965253472328186),\n",
       " ('hostess', 0.48585087060928345),\n",
       " ('take-alongs', 0.44097140431404114),\n",
       " ('multitasking', 0.4398477375507355),\n",
       " ('lunch', 0.43938422203063965),\n",
       " ('night', 0.4361981153488159),\n",
       " ('starter', 0.43114975094795227)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_it.wv.most_similar('dinner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('meal', 0.6799939274787903),\n",
       " ('supper', 0.5672358274459839),\n",
       " ('lunch', 0.4771226942539215),\n",
       " ('snack', 0.4711977243423462),\n",
       " ('dinners', 0.4625163674354553),\n",
       " ('night', 0.43596264719963074),\n",
       " ('entertaining', 0.41720816493034363),\n",
       " ('multitasking', 0.40387624502182007),\n",
       " ('hostess', 0.40293624997138977),\n",
       " ('occasion', 0.3939659595489502)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_ch.wv.most_similar('dinner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'spaghetti'\n",
    "v0, vit, vch = m0.wv.get_vector(word), m_it.wv.get_vector(word), m_ch.wv.get_vector(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving to IT: 0.06417824268192884\n",
      "Moving to CH: 0.05927230830607866\n",
      "Moving from IT to CH: 0.13013635178913807\n"
     ]
    }
   ],
   "source": [
    "print(f\"Moving to IT: {distance.cosine(vit, v0)}\")\n",
    "print(f\"Moving to CH: {distance.cosine(vch, v0)}\")\n",
    "print(f\"Moving from IT to CH: {distance.cosine(vch, vit)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gensim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
